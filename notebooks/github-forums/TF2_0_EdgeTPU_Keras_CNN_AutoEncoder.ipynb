{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "qLXmBB9bQoT7",
    "outputId": "f1372a05-94df-4026-d49f-283cd8c3a14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
      "Executing: /tmp/apt-key-gpghome.xLwbjpgTJz/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 6A030B21BA07F4FB\n",
      "Requirement already satisfied: tensorflow-gpu==2.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.11.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (3.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.0.8)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.14.0a20190603)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.16.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (0.33.4)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (0.7.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (0.1.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0-beta1) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0-beta1) (41.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0-beta1) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0-beta1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0-beta1) (0.15.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "gpg: key 6A030B21BA07F4FB: \"Google Cloud Packages Automatic Signing Key <gc-team@google.com>\" not changed\n",
      "gpg: Total number processed: 1\n",
      "gpg:              unchanged: 1\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# install edgetpu_compiler\n",
    "echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 6A030B21BA07F4FB\n",
    "\n",
    "sudo apt update > /dev/null\n",
    "sudo apt install edgetpu > /dev/null\n",
    "\n",
    "# install tensorflow 2.0\n",
    "pip install tensorflow-gpu==2.0-beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDYJKuhfRoyo"
   },
   "source": [
    "## Build CNN AutoEncoder with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3aluRnFKRN9y",
    "outputId": "d31e52f3-8fd2-452f-cd5a-67a568c3c2fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow's version: 2.0.0-beta1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(f'TensorFlow\\'s version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1TSe-d1m9o4"
   },
   "source": [
    "In this notebook we use Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p40JjhzMSU92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "(60000, 28, 28)\n",
      "uint8\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print(train_images.dtype)\n",
    "print(train_images.shape)\n",
    "print(test_images.dtype)\n",
    "print(test_images.shape)\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_images = train_images[:, :, :, np.newaxis].astype(np.float32)\n",
    "test_images = test_images[:, :, :, np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhC7xCG6S-yD"
   },
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pIowyx-yB8J"
   },
   "outputs": [],
   "source": [
    "# UpSampling2D (using nearest neighber) is not supported yet.\n",
    "# We should implement it by myself.\n",
    "def UpSampling2D(scale=(2, 2)):\n",
    "    if isinstance(scale, int):\n",
    "        scale = (scale, scale)\n",
    "\n",
    "    def upsampling(x):\n",
    "        shape = x.shape\n",
    "        x = keras.layers.Concatenate(-2)([x]*scale[0])\n",
    "        x = keras.layers.Reshape([shape[1]*scale[0], shape[2], shape[3]])(x)\n",
    "        x = keras.layers.Concatenate(-1)([x]*scale[1])\n",
    "        x = keras.layers.Reshape([shape[1]*scale[0], shape[2]*scale[1], shape[3]])(x)\n",
    "        return x\n",
    "    \n",
    "    return upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNuqMwp5f_GX"
   },
   "outputs": [],
   "source": [
    "def build_keras_model(hidden_size=2):\n",
    "    inputs = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(16, (3, 3), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(8, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(8, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    encoded = layers.Dense(hidden_size)(x)\n",
    "    \n",
    "    x = layers.Dense(7*7*8)(encoded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Reshape((7, 7, 8))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    # x = layers.UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    x = layers.Conv2D(8, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    # x = layers.UpSampling2D(2, interpolation='bilinear')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = models.Model(inputs, decoded)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1465
    },
    "colab_type": "code",
    "id": "jUuyAcC8ypv6",
    "outputId": "11683d0d-1812-4006-8d15-b258287103bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 20:02:16.452505 140428756795520 deprecation.py:323] From /home/maka/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 28, 28, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 28, 28, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 14, 14, 16)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 8)    1160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14, 14, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14, 14, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 8)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 8)      584         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 8)      32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7, 7, 8)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 392)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            786         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 392)          1176        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 392)          1568        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 392)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 7, 7, 8)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 7, 14, 8)     0           reshape[0][0]                    \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 14, 7, 8)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 7, 16)    0           reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 14, 14, 8)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 8)    584         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 14, 14, 8)    32          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 14, 14, 8)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 28, 8)    0           activation_4[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 28, 14, 8)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 28, 14, 16)   0           reshape_3[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 28, 28, 8)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 16)   1168        reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 28, 28, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 1)    145         activation_5[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,555\n",
      "Trainable params: 6,659\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 22s 367us/sample - loss: 0.3650\n"
     ]
    }
   ],
   "source": [
    "model = build_keras_model()\n",
    "model.summary()\n",
    "model.fit(train_images, train_images, epochs=1)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTkI6OXfnYxy"
   },
   "source": [
    "We need small dataset to calibrate quantization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BaalYeDaaPm8",
    "outputId": "0eb6fa8a-3a06-4e8e-9899-053ab19e2c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(50000, 32, 32, 3)\n",
      "float64\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "print(train_images.dtype)\n",
    "print(train_images.shape)\n",
    "print(test_images.dtype)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array([\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm'\n",
    "])\n",
    "def display(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "import time \n",
    "def compile_(model):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "def test(model):\n",
    "    now = time.time()\n",
    "    test_loss, test_acc = basic_model.evaluate(test_images, test_labels)\n",
    "    elapsed = time.time() - now\n",
    "    print('[%.3fs] Test accuracy: %s' % (elapsed, test_acc))\n",
    "\n",
    "def predict(model):\n",
    "    now = time.time()\n",
    "    model.predict(test_images)\n",
    "    elapsed = time.time() - now\n",
    "    return elapsed\n",
    "\n",
    "def time_predict(model, times):\n",
    "    predict_times = [predict(model) for _ in range(10)]\n",
    "    mean = (sum(predict_times) - min(predict_times) - max(predict_times)) / (len(predict_times) - 2)\n",
    "    print('Prediction Timing: %.3fs' % mean)\n",
    "\n",
    "def fit(model, epochs_=1, initial_epoch=0):\n",
    "    model.fit(x=train_images, y=train_labels, epochs=epochs_, initial_epoch=initial_epoch)\n",
    "\n",
    "def save(model, name, epochs_):\n",
    "    filename = name + '_%03d.h5' % epochs_\n",
    "    print('Saving %s...' % filename)\n",
    "    model.save(filename)\n",
    "\n",
    "def train(name, model, epochs_, initial_epoch=0):\n",
    "    for i in range(initial_epoch + 1, epochs_ + 1):\n",
    "        label = str(i) + ' ' + name\n",
    "        print('~~~~~~~~~~~~~~~~~~~~ ', label, ' ~~~~~~~~~~~~~~~~~~~~', sep='')\n",
    "        \n",
    "        # fit\n",
    "        fit(model, i, initial_epoch=i-1)\n",
    "        \n",
    "        # predict\n",
    "        time_predict(model, 5)\n",
    "\n",
    "        # test\n",
    "        test(model)\n",
    "        \n",
    "        # save\n",
    "        save(model, name, i)\n",
    "        print('~~~~~~~~~~~~~~~~~~~~ ', '/end '.ljust(len(label) + 1, '='), '~~~~~~~~~~~~~~~~~~~~', sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 406,244\n",
      "Trainable params: 406,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 4s 76us/sample - loss: 4.4028 - accuracy: 0.0242\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 4.3188 - accuracy: 0.0260\n",
      "[0.580s] Test accuracy: 0.026\n"
     ]
    }
   ],
   "source": [
    "basic_model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=train_images.shape[1:]),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(len(class_names), activation=tf.nn.softmax)\n",
    "])\n",
    "basic_model.summary()\n",
    "compile_(basic_model)\n",
    "fit(basic_model, 1)\n",
    "test(basic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BaalYeDaaPm8",
    "outputId": "0eb6fa8a-3a06-4e8e-9899-053ab19e2c30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0805 20:50:09.696093 140428756795520 convert_to_constants.py:238] Converted 320 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset_gen():\n",
    "    for i in range(1000):\n",
    "        result = train_images[i: i + 1].astype(np.float32)\n",
    "        yield [result]\n",
    "        \n",
    "\n",
    "# Currentry, it seems that tf.lite.TFLiteConverter does not suppert inference_input/output_type yet.\n",
    "# So we have to use tf.compat.v1.lite.TFLiteConverter.\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('resnet50_001.h5')\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open('model.tflite', 'wb') as o_:\n",
    "    o_.write(tflite_model)\n",
    "    \n",
    "    \n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.representative_dataset = representative_dataset_gen\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# with open('model.tflite', 'wb') as o_:\n",
    "#     o_.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56tpcmlBoLNY"
   },
   "source": [
    "We can validate the tflite model using tf.lite.Interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "colab_type": "code",
    "id": "KRn3RZZLiYMw",
    "outputId": "dcd1f738-7ea6-4924-aabd-0fadc831b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30313634872436523\n",
      "0.01671123504638672\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "#input_shape = input_details[0]['shape']\n",
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\n",
    "now = time.time()\n",
    "for _ in range(1):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(time.time() - now)\n",
    "\n",
    "now = time.time()\n",
    "basic_model.predict(test_images[:1])\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "colab_type": "code",
    "id": "KRn3RZZLiYMw",
    "outputId": "dcd1f738-7ea6-4924-aabd-0fadc831b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[198 214 248]\n",
      "  [195 210 243]\n",
      "  [194 209 242]\n",
      "  ...\n",
      "  [215 230 249]\n",
      "  [216 230 249]\n",
      "  [223 233 251]]\n",
      "\n",
      " [[196 209 238]\n",
      "  [194 207 237]\n",
      "  [194 209 239]\n",
      "  ...\n",
      "  [230 242 249]\n",
      "  [232 242 249]\n",
      "  [240 244 252]]\n",
      "\n",
      " [[221 225 245]\n",
      "  [212 219 241]\n",
      "  [208 218 242]\n",
      "  ...\n",
      "  [242 249 250]\n",
      "  [243 248 250]\n",
      "  [249 249 252]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 71  72  98]\n",
      "  [ 70  73 101]\n",
      "  [ 73  77 107]\n",
      "  ...\n",
      "  [219 207 216]\n",
      "  [182 167 180]\n",
      "  [154 140 149]]\n",
      "\n",
      " [[ 71  74 103]\n",
      "  [ 75  80 110]\n",
      "  [ 83  88 121]\n",
      "  ...\n",
      "  [221 211 219]\n",
      "  [186 173 191]\n",
      "  [144 131 148]]\n",
      "\n",
      " [[ 79  84 117]\n",
      "  [ 83  89 122]\n",
      "  [ 84  91 126]\n",
      "  ...\n",
      "  [216 206 214]\n",
      "  [206 193 210]\n",
      "  [175 163 182]]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "There is at least 1 reference to internal data\n      in the interpreter in the form of a numpy array or slice. Be sure to\n      only hold the function returned from tensor() if you are using raw\n      data access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-9b14a85edf10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#input_().fill(quantize(test_images[i]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minput_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inference %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mfails\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/deep-learning/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m_ensure_safe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mBe\u001b[0m \u001b[0msure\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0monly\u001b[0m \u001b[0mhold\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myou\u001b[0m \u001b[0mare\u001b[0m \u001b[0musing\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m       data access.\"\"\")\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There is at least 1 reference to internal data\n      in the interpreter in the form of a numpy array or slice. Be sure to\n      only hold the function returned from tensor() if you are using raw\n      data access."
     ]
    }
   ],
   "source": [
    "def quantize(real_value):\n",
    "    std, mean = input_detail['quantization']\n",
    "    return (real_value/std + mean).astype(np.uint8)\n",
    "\n",
    "\n",
    "#interpreter.allocate_tensors()\n",
    "input_ = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])\n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "for i in range(10):\n",
    "    input_().fill(3.)\n",
    "    interpreter.invoke()\n",
    "    print(\"inference %s\" % output())\n",
    "\n",
    "sample_input = quantize(test_images)\n",
    "print(sample_input[0])\n",
    "\n",
    "interpreter.set_tensor(input_detail['index'], sample_input)\n",
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "colab_type": "code",
    "id": "KRn3RZZLiYMw",
    "outputId": "dcd1f738-7ea6-4924-aabd-0fadc831b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'flatten_input', 'index': 8, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0)}]\n",
      "[{'name': 'Identity', 'index': 9, 'shape': array([  1, 100], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\n",
      "[{'name': 'Identity', 'index': 9, 'shape': array([  1, 100], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "[49]\n",
      "(1, 32, 32, 3)\n",
      "plain\n",
      "[[8.77413608e-04 4.37755743e-03 2.26805243e-03 6.87796855e-03\n",
      "  6.71930145e-03 1.16651002e-02 1.37637649e-03 2.30081868e-03\n",
      "  9.86437872e-03 7.54444953e-03 6.22065226e-03 5.13636181e-03\n",
      "  2.62515768e-02 1.96711589e-02 1.32081716e-03 1.73578821e-02\n",
      "  2.42905947e-03 5.37013523e-02 4.94780764e-03 1.53299384e-02\n",
      "  2.80757621e-03 8.11393932e-03 6.19577290e-03 1.77304614e-02\n",
      "  2.31868890e-03 1.69448089e-02 2.45091785e-03 6.20003743e-03\n",
      "  3.14367213e-03 6.92004245e-03 3.56732286e-03 1.12142731e-02\n",
      "  2.12131571e-02 1.77665297e-02 1.32757402e-03 2.95896037e-03\n",
      "  2.26460863e-03 2.93512456e-02 2.12090136e-03 1.00549953e-02\n",
      "  1.18680308e-02 1.35544455e-02 3.88921704e-03 9.38796613e-04\n",
      "  3.00852838e-03 2.56824796e-03 1.52746579e-02 3.42156403e-02\n",
      "  1.09328888e-02 1.10569801e-02 1.57715171e-03 4.40639630e-03\n",
      "  1.44731952e-02 4.94472588e-05 3.40132532e-03 9.67041682e-03\n",
      "  1.34621644e-02 2.77635595e-03 2.24555898e-02 1.60455406e-02\n",
      "  6.01551645e-02 1.95857347e-03 6.14114513e-04 1.93495606e-03\n",
      "  2.90116668e-03 4.46749665e-03 9.29957954e-04 1.17990794e-02\n",
      "  1.61360595e-02 1.39823332e-02 6.62205217e-04 3.69388536e-02\n",
      "  1.26384078e-02 7.46288616e-03 1.07183261e-03 1.66954275e-03\n",
      "  1.31833684e-02 5.04391780e-03 1.11103116e-03 4.50760731e-03\n",
      "  2.66515766e-03 3.40684354e-02 8.33311304e-03 1.80314656e-03\n",
      "  8.85880366e-03 2.39964668e-02 4.63007158e-03 9.59198643e-03\n",
      "  6.50470844e-04 2.94931009e-02 2.92010643e-02 5.11233555e-03\n",
      "  6.52262359e-04 1.48166744e-02 1.24677282e-03 2.80155311e-03\n",
      "  3.51856537e-02 1.38972909e-03 3.53892124e-03 6.26718765e-03]]\n",
      "bear\n",
      "[[2 2 2 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 2 2\n",
      "  2 3 3 3 2 3 3 2 2 2 2 3 3 3 3 2 3 2 2 3 3 3 3 3 3 2 2 3 2 2 2 3 3 3 2 3\n",
      "  3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 2 2 3 3 3 3 2 2 3]]\n",
      "['apple' 'aquarium_fish' 'baby' 'bear' 'beaver' 'bed' 'bee' 'beetle'\n",
      " 'bicycle' 'bottle' 'bowl' 'boy' 'bridge' 'bus' 'butterfly' 'camel' 'can'\n",
      " 'castle' 'caterpillar' 'cattle' 'chair' 'chimpanzee' 'clock' 'cloud'\n",
      " 'cockroach' 'couch' 'crab' 'crocodile' 'cup' 'dinosaur' 'dolphin'\n",
      " 'elephant' 'flatfish' 'forest' 'fox' 'girl' 'hamster' 'house' 'kangaroo'\n",
      " 'keyboard' 'lamp' 'lawn_mower' 'leopard' 'lion' 'lizard' 'lobster' 'man'\n",
      " 'maple_tree' 'motorcycle' 'mountain' 'mouse' 'mushroom' 'oak_tree'\n",
      " 'orange' 'orchid' 'otter' 'palm_tree' 'pear' 'pickup_truck' 'pine_tree'\n",
      " 'plain' 'plate' 'poppy' 'porcupine' 'possum' 'rabbit' 'raccoon' 'ray'\n",
      " 'road' 'rocket' 'rose' 'sea' 'seal' 'shark' 'shrew' 'skunk' 'skyscraper'\n",
      " 'snail' 'snake' 'spider' 'squirrel' 'streetcar' 'sunflower'\n",
      " 'sweet_pepper' 'table' 'tank' 'telephone' 'television' 'tiger' 'tractor'\n",
      " 'train' 'trout' 'tulip' 'turtle' 'wardrobe' 'whale' 'willow_tree' 'wolf'\n",
      " 'woman' 'worm']\n"
     ]
    }
   ],
   "source": [
    "original_image = test_images[0]\n",
    "display(original_image)\n",
    "print(test_images[0].shape)\n",
    "print(test_labels[0])\n",
    "print(test_images[:1].shape)\n",
    "\n",
    "pred_original_model = basic_model.predict(test_images[:1])\n",
    "print(class_names[np.argmax(pred_original_model)])\n",
    "print(pred_original_model)\n",
    "\n",
    "pred_quantized_model =  interpreter.get_tensor(output_detail['index'])\n",
    "print(class_names[np.argmax(pred_quantized_model)])\n",
    "print(pred_quantized_model)\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Timing: 0.493s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict(model):\n",
    "    now = time.time()\n",
    "    model.predict(test_images)\n",
    "    elapsed = time.time() - now\n",
    "    return elapsed\n",
    "\n",
    "def time_predict(model, times=5):\n",
    "    predict_times = [predict(model) for _ in range(10)]\n",
    "    mean = (sum(predict_times) - min(predict_times) - max(predict_times)) / (len(predict_times) - 2)\n",
    "    print('Prediction Timing: %.3fs' % mean)\n",
    "    \n",
    "time_predict(basic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "GmSX6C6RxAZh",
    "outputId": "0db157eb-19f9-48b5-eaa4-88cae4d3ed62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge TPU Compiler version 1.0.249710469\n",
      "\n",
      "Model compiled successfully in 89 ms.\n",
      "\n",
      "Input model: model.tflite\n",
      "Input size: 15.63KiB\n",
      "Output model: model_edgetpu.tflite\n",
      "Output size: 132.55KiB\n",
      "On-chip memory available for caching model parameters: 7.82MiB\n",
      "On-chip memory used for caching model parameters: 57.50KiB\n",
      "Off-chip memory used for streaming uncached model parameters: 0.00B\n",
      "Number of Edge TPU subgraphs: 1\n",
      "Total number of operations: 22\n",
      "Operation log: model_edgetpu.log\n",
      "\n",
      "Operator                       Count      Status\n",
      "\n",
      "CONCATENATION                  4          Mapped to Edge TPU\n",
      "QUANTIZE                       2          Mapped to Edge TPU\n",
      "CONV_2D                        5          Mapped to Edge TPU\n",
      "DEPTHWISE_CONV_2D              1          Mapped to Edge TPU\n",
      "RESHAPE                        5          Mapped to Edge TPU\n",
      "FULLY_CONNECTED                2          Mapped to Edge TPU\n",
      "LOGISTIC                       1          Mapped to Edge TPU\n",
      "MAX_POOL_2D                    2          Mapped to Edge TPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initialized TensorFlow Lite runtime.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "edgetpu_compiler --show_operations 'model.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygUHjCLOz41Q"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('model_edgetpu.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nlq_cUMJkuSC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF2.0 EdgeTPU Keras CNN AutoEncoder",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
